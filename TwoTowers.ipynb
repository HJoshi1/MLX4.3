{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Analogue of the nn.RNN module\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, nonlinearity='tanh'):\n",
    "        super(MyRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(num_layers, hidden_size, input_size))\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(num_layers, hidden_size, hidden_size))\n",
    "        if bias:\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(num_layers, hidden_size))\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(num_layers, hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / (self.hidden_size ** 0.5)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        '''\n",
    "        This function defines a forward RNN pass  \n",
    "\n",
    "        Input: tensor of shape (batch_size, sequence_length, input_size)'\n",
    "        Output: (output, hx) where output is a list of tensors oh  cell\n",
    "        predictions, shape (num_layers, batch_size, hidden_size)\n",
    "        '''\n",
    "        # Initializes the hidden state if not provided\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(self.num_layers, input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        #iterate over each time step\n",
    "        for i in range(input.size(1)):\n",
    "            hx = self.rnn_cell(input[:, i, :], hx)\n",
    "            outputs.append(hx.unsqueeze(1))\n",
    "\n",
    "        output = torch.cat(outputs, dim=1)\n",
    "        return output, hx\n",
    "\n",
    "    def rnn_cell(self, input, hx):\n",
    "        '''\n",
    "        Defines a run of one RNN batch for one time step\n",
    "\n",
    "        Inputs: \n",
    "            input tensor of hape (batch_size, 1, input_size)\n",
    "            hx tensor of shape (num_layers, batch_size, hidden_size)\n",
    "        Output:\n",
    "            tensor of shape (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        '''\n",
    "        # Apply RNN cell computation  --> tensor (batch_size, hidden_size)\n",
    "        gates = torch.matmul(input, self.weight_ih.transpose(0, 1)) + torch.matmul(hx, self.weight_hh.transpose(0, 1))\n",
    "        if self.bias_ih is not None:\n",
    "            gates += self.bias_ih.unsqueeze(0)\n",
    "            gates += self.bias_hh.unsqueeze(0)\n",
    "        if self.nonlinearity == 'tanh':\n",
    "            return torch.tanh(gates)\n",
    "        elif self.nonlinearity == 'relu':\n",
    "            return torch.relu(gates)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported nonlinearity. Choose from 'tanh' or 'relu'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "class fullRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(fullRNN, self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.rnn_cell=nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer=nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        This functions defines forward prop through our RNN network.\n",
    "        The input is a tensor of shape (seq_length, batch_size, input_size)\n",
    "        The seq_length is number of examples\n",
    "        '''\n",
    "        print(input.size())\n",
    "        \n",
    "        #Initiates the hidden layer for the whole text\n",
    "        hidden = torch.zeros(1, input.size(0), self.hidden_size)\n",
    "        print(hidden.size())\n",
    "        # hidden=torch.zeros (input.size(1), self.hidden_size)\n",
    "        rnn_output, hidden = self.rnn_cell(input, hidden)\n",
    "        output=self.output_layer(rnn_output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# input_size = 128\n",
    "# hidden_size = 100\n",
    "# output_size = 100\n",
    "\n",
    "# # Step 1 - Create RNN for Query Tower + for Doc tower \n",
    "# queryRNN = fullRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# # Step 2 - Load input data - pickle files have been tokenised by sentence piece and embedded by\n",
    "# # Data in format - [tesnor([tensor(query), tensor(rel_docs), tensor(irr_docs)]), .... ]\n",
    "# # For query\n",
    "# testData = []\n",
    "# trainingData = [] \n",
    "# # To prep the data\n",
    "# validationData = []\n",
    "\n",
    "# with open('test_tokenised_triplets.pkl', 'rb') as file:\n",
    "#     testData = pickle.load(file)\n",
    "# with open('training_tokenised_triplets.pkl', 'rb') as file:\n",
    "#     trainingData = pickle.load(file)\n",
    "# with open('validation_tokenised_triplets.pkl', 'rb') as file:\n",
    "#     validationData = pickle.load(file)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Get the dataset from MS Marco and put into data frame\n",
    "\n",
    "training_query_dataset = pd.read_parquet(\"./v1.1-data/train.parquet\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Take the query out of the triplet \n",
    "# query_list = []\n",
    "# relevant_doc_list = []\n",
    "# irrelevant_doc_list = []\n",
    "# for (query, relevant_doc, irrelevant_doc) in trainingData:\n",
    "# # This gives tensor([w1, w2, w3,...wn]) for each individual query\n",
    "# # Take the query out of a tensor form and keep as a list\n",
    "# # Then iterate over all of the triplets and pull them all out\n",
    "#     query_list.append(query)\n",
    "#     relevant_doc_list.append(relevant_doc)  ## TODO: don't tensorise in the first place\n",
    "#     irrelevant_doc_list.append(irrelevant_doc) ## TODO: don't tensorise in the first place\n",
    "# # Put them all in one list \n",
    "# # Make this list a tensor\n",
    "\n",
    "# def pad(list_of_arrays):\n",
    "#     # Find the length of the longest list\n",
    "#     max_length = 0\n",
    "#     for lst in list_of_arrays:\n",
    "#         if len(lst) > max_length:\n",
    "#             max_length = len(lst)\n",
    "            \n",
    "#     # Pad all lists to the same length\n",
    "#     padded_arrays = []\n",
    "#     for lst in list_of_arrays:\n",
    "#         arr = np.array(lst)\n",
    "#         padded_arr = np.pad(arr, ((max_length - len(arr), 0), (0, 0)), mode='constant', constant_values=0)\n",
    "#         padded_arrays.append(padded_arr)\n",
    "#     return padded_arrays\n",
    "\n",
    "# padded_queries = pad(query_list)\n",
    "\n",
    "# for i in range(len(query_list)):\n",
    "#     print(np.array(query_list[i]).shape)\n",
    "    \n",
    "# for i in range(len(padded_queries)):\n",
    "#     print(padded_queries[i].shape)\n",
    "\n",
    "# query_list = torch.tensor(padded_queries).to(torch.float32)\n",
    "# relevant_doc_list = torch.tensor(pad(relevant_doc_list)).to(torch.float32)\n",
    "# irrelevant_doc_list = torch.tensor(pad(irrelevant_doc_list)).to(torch.float32)\n",
    "\n",
    "\n",
    "def splitTripletIntoIndividualElements(trainingData):\n",
    "    query_list = []\n",
    "    relevant_doc_list = []\n",
    "    irrelevant_doc_list = []\n",
    "    for (query, relevant_doc, irrelevant_doc) in trainingData:\n",
    "    # This gives tensor([w1, w2, w3,...wn]) for each individual query\n",
    "    # Take the query out of a tensor form and keep as a list\n",
    "    # Then iterate over all of the triplets and pull them all out\n",
    "        query_list.append(query)\n",
    "        relevant_doc_list.append(relevant_doc)  ## TODO: don't tensorise in the first place\n",
    "        irrelevant_doc_list.append(irrelevant_doc) ## TODO: don't tensorise in the first place\n",
    "    # Put them all in one list \n",
    "    # Make this list a tensor\n",
    "\n",
    "    def pad(list_of_arrays):\n",
    "        # Find the length of the longest list\n",
    "        max_length = 0\n",
    "        for lst in list_of_arrays:\n",
    "            if len(lst) > max_length:\n",
    "                max_length = len(lst)\n",
    "                \n",
    "        # Pad all lists to the same length\n",
    "        padded_arrays = []\n",
    "        for lst in list_of_arrays:\n",
    "            arr = np.array(lst)\n",
    "            padded_arr = np.pad(arr, ((max_length - len(arr), 0), (0, 0)), mode='constant', constant_values=0)\n",
    "            padded_arrays.append(padded_arr)\n",
    "        return padded_arrays\n",
    "\n",
    "    padded_queries = pad(query_list)\n",
    "\n",
    "    for i in range(len(query_list)):\n",
    "        print(np.array(query_list[i]).shape)\n",
    "        \n",
    "    for i in range(len(padded_queries)):\n",
    "        print(padded_queries[i].shape)\n",
    "\n",
    "    query_list = torch.tensor(padded_queries).to(torch.float32)\n",
    "    relevant_doc_list = torch.tensor(pad(relevant_doc_list)).to(torch.float32)\n",
    "    irrelevant_doc_list = torch.tensor(pad(irrelevant_doc_list)).to(torch.float32)\n",
    "\n",
    "    return query_list, relevant_doc_list, irrelevant_doc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([814, 18, 128])\n",
      "torch.Size([1, 814, 100])\n"
     ]
    }
   ],
   "source": [
    "# # Step 3 - Pass data into model \n",
    "# criterion = nn.NLLLoss()\n",
    "# learning_rate = 0.005 # param, play around with to learn\n",
    "# optimizer = torch.optim.SGD(queryRNN.parameters(), lr=learning_rate) #stochastic gradient descent\n",
    "\n",
    "# query_output = queryRNN(query_list)\n",
    "# # output.forward(trainintensor_query_listData[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([814, 12, 128])\n",
      "torch.Size([1, 814, 100])\n",
      "torch.Size([814, 12, 128])\n",
      "torch.Size([1, 814, 100])\n"
     ]
    }
   ],
   "source": [
    "# # Step 4 - Create Document RNN\n",
    "# documentRNN = fullRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# # Step 5 - Run Document RNN\n",
    "# relevant_output = documentRNN(relevant_doc_list)\n",
    "# irrelevant_output = documentRNN(irrelevant_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Step 6 - Create Loss function\n",
    "# margin = 0.3\n",
    "\n",
    "# def difference_function(vector1, vector2):\n",
    "#     return F.cosine_similarity(vector1, vector2)\n",
    "\n",
    "# def triplet_loss_function(query_output, relevant_output, irrelevant_output, margin):\n",
    "#     relevant_similarity = difference_function(query_output, relevant_output)\n",
    "#     irrelevant_similarity = difference_function(query_output, irrelevant_output)\n",
    "#     triplet_loss = torch.max(torch.tensor(0), margin + relevant_similarity - irrelevant_similarity)\n",
    "#     print(triplet_loss)\n",
    "#     return triplet_loss.mean()\n",
    "\n",
    "# loss = triplet_loss_function(query_output, relevant_output, irrelevant_output, margin)\n",
    "\n",
    "# print(loss)\n",
    "\n",
    "# # Step 7 - Backpropogate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two towers architecture \n",
    "\n",
    "class TwoTowers(nn.Module):\n",
    "    def __init__(self, queryTower, documentTower, margin):\n",
    "        super(fullRNN, self).__init__()\n",
    "        self.queryTower = queryTower\n",
    "        self.documentTower = documentTower\n",
    "        self.margin = margin\n",
    "\n",
    "    def difference_function(vector1, vector2):\n",
    "        return F.cosine_similarity(vector1, vector2)\n",
    "\n",
    "    def triplet_loss_function(self, query_output, relevant_output, irrelevant_output):\n",
    "        relevant_similarity = self.difference_function(query_output, relevant_output)\n",
    "        irrelevant_similarity = self.difference_function(query_output, irrelevant_output)\n",
    "        triplet_loss = torch.max(torch.tensor(0), self.margin + relevant_similarity - irrelevant_similarity)\n",
    "        return triplet_loss.mean()\n",
    "\n",
    "    def forward(self, query_list, relevant_doc_list, irrelevant_doc_list):\n",
    "        query_output = self.queryTower(query_list)\n",
    "        relevant_output = self.documentTower(relevant_doc_list)\n",
    "        irrelevant_output = self.documentTower(irrelevant_doc_list)\n",
    "        score = self.triplet_loss_function(query_output, relevant_output, irrelevant_output, self.margin)\n",
    "        return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Two Towers architecture and train\n",
    "\n",
    "input_size = 128\n",
    "hidden_size = 100\n",
    "output_size = 100\n",
    "\n",
    "# Step 1 - Create RNN for Query Tower + for Doc tower \n",
    "queryRNN = fullRNN(input_size, hidden_size, output_size)\n",
    "documentRNN = fullRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "margin = 0.5\n",
    "two_towers = TwoTowers(queryRNN, documentRNN, margin)\n",
    "\n",
    "# Step 2 - Load input data - pickle files have been tokenised by sentence piece and embedded by\n",
    "# Data in format - [tesnor([tensor(query), tensor(rel_docs), tensor(irr_docs)]), .... ]\n",
    "# For query\n",
    "testData = []\n",
    "trainingData = [] \n",
    "# To prep the data\n",
    "validationData = []\n",
    "\n",
    "with open('test_tokenised_triplets.pkl', 'rb') as file:\n",
    "    testData = pickle.load(file)\n",
    "with open('training_tokenised_triplets.pkl', 'rb') as file:\n",
    "    trainingData = pickle.load(file)\n",
    "with open('validation_tokenised_triplets.pkl', 'rb') as file:\n",
    "    validationData = pickle.load(file)\n",
    "\n",
    "\n",
    "query_list, relevant_doc_list, irrelevant_doc_list = splitTripletIntoIndividualElements(validationData)\n",
    "\n",
    "# Step 3 - Pass data into model \n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005 # param, play around with to learn\n",
    "optimizer = torch.optim.SGD(queryRNN.parameters(), lr=learning_rate) #stochastic gradient descent\n",
    "\n",
    "query_output = queryRNN(query_list)\n",
    "\n",
    "# Step 4 - Create Document RNN\n",
    "# documentRNN = fullRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Step 5 - Run Document RNN\n",
    "relevant_output = documentRNN(relevant_doc_list)\n",
    "irrelevant_output = documentRNN(irrelevant_doc_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Give it the training data for training \n",
    "# Train model \n",
    "# Do valdation stuff\n",
    "# Test with test data \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/harshil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch\n",
    "import nn.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Get the dataset from MS Marco and put into data frame\n",
    "\n",
    "training_query_dataset = pd.read_parquet(\"./v1.1-data/train.parquet\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2 - Compile list of all the documenets \n",
    "# This step removes the redundant columns from the data frame so we're left with the information we want - we haven't formatted anything \n",
    "# at this point.\n",
    "\n",
    "# query_and_relevant_doc_df = query_dataset.drop(columns=[\"answers\", \"query_id\", \"query_type\", \"wellFormedAnswers\"])\n",
    "\n",
    "def removeRedundantColumns(dataset):\n",
    "    return dataset.drop(columns=[\"answers\", \"query_id\", \"query_type\", \"wellFormedAnswers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Generate triplets \n",
    "# - form: (query, relevant_doc, irrelevant_doc)\n",
    "# Relevant docs are stored with the query in the df above\n",
    "# Irrelevant doc is a random document from any other query in the df - IMPORTANT - WE'RE ASSUMING THE OTHER QUERY IS UNRELATED TO THIS ONE\n",
    "\n",
    "# storing in a dictionairy of form (k,v) - k = query, v = [(rel_doc1, irr_doc1), (rel_doc2, irr_doc2)....,] - this is our negative sampling\n",
    "# query_dataset.head(1).loc[:,\"passages\"][0]['passage_text'][4]\n",
    "\n",
    "\n",
    "\n",
    "# triplets_list = []\n",
    "# num_rows = query_and_relevant_doc_df.shape[0]\n",
    "\n",
    "# # Reset file\n",
    "# with open('corpus.txt', 'w') as file:    \n",
    "#     file.write(\"\") \n",
    "\n",
    "\n",
    "# for index, row in query_and_relevant_doc_df.iterrows():\n",
    "#     query = row[\"query\"]  # Assuming 'query' is the column name for the query text\n",
    "#     passages = row[\"passages\"]  # Accessing the dictionary in the 'passages' column\n",
    "#     passage_texts = passages[\"passage_text\"]  # Extracting the list of passage texts\n",
    "\n",
    "    \n",
    "#     for relevant_document in passage_texts:        \n",
    "#         # Randomly select another index\n",
    "#         random_row_index = random.randint(0, num_rows - 1)\n",
    "#         # Spliting each passage into a sentence \n",
    "#         sentences = sent_tokenize(relevant_document)\n",
    "#         # appending each sentence into corpus.txt\n",
    "#         with open('corpus.txt', 'a') as file:\n",
    "#             for sentence in sentences:\n",
    "#                 file.write(sentence + '\\n')  # Write each sentence on a new line\n",
    "\n",
    "#         while index == random_row_index:\n",
    "#             random_row_index = random.randint(0, num_rows - 1)\n",
    "\n",
    "        \n",
    "#         # Retrieve a passage from the randomly selected row\n",
    "#         random_passages = query_and_relevant_doc_df.loc[random_row_index, \"passages\"]\n",
    "#         random_passage_texts = random_passages[\"passage_text\"]\n",
    "        \n",
    "#         # Optionally, select a random passage text from the selected row\n",
    "#         irrelevant_document = random.choice(random_passage_texts)\n",
    "#         triplets_list.append((query, relevant_document, irrelevant_document))\n",
    "\n",
    "\n",
    "def clearCorpusFile():\n",
    "    with open('corpus.txt', 'w') as file:    \n",
    "        file.write(\"\") \n",
    "\n",
    "\n",
    "def writeToCorpusAndGenerateTriplets(query_and_relevant_doc_df):\n",
    "    triplets_list = []\n",
    "    num_rows = query_and_relevant_doc_df.shape[0]\n",
    "\n",
    "    for index, row in query_and_relevant_doc_df.iterrows():\n",
    "        query = row[\"query\"]  # Assuming 'query' is the column name for the query text\n",
    "        passages = row[\"passages\"]  # Accessing the dictionary in the 'passages' column\n",
    "        passage_texts = passages[\"passage_text\"]  # Extracting the list of passage texts\n",
    "\n",
    "        \n",
    "        for relevant_document in passage_texts:        \n",
    "            # Randomly select another index\n",
    "            random_row_index = random.randint(0, num_rows - 1)\n",
    "            # Spliting each passage into a sentence \n",
    "            rel_doc_sentences_list = sent_tokenize(relevant_document)\n",
    "            # appending each sentence into corpus.txt\n",
    "            with open('corpus.txt', 'a') as file:\n",
    "                for sentence in rel_doc_sentences_list:\n",
    "                    file.write(sentence + '\\n')  # Write each sentence on a new line\n",
    "\n",
    "            while index == random_row_index:\n",
    "                random_row_index = random.randint(0, num_rows - 1)\n",
    "\n",
    "            \n",
    "            # Retrieve a passage from the randomly selected row\n",
    "            random_passages = query_and_relevant_doc_df.loc[random_row_index, \"passages\"]\n",
    "            random_passage_texts = random_passages[\"passage_text\"]\n",
    "            \n",
    "            # Optionally, select a random passage text from the selected row\n",
    "            irrelevant_document = random.choice(random_passage_texts)\n",
    "            irrel_doc_sentences_list = sent_tokenize(irrelevant_document)\n",
    "            triplets_list.append((query, rel_doc_sentences_list, irrel_doc_sentences_list))\n",
    "    return triplets_list\n",
    "\n",
    "    \n",
    "\n",
    "# - create triplet for each of the doc values\n",
    "# - randomly choose a doc from other keys\n",
    "# - store somewhere \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1 Train the sentencepiece model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4.2 Load the trained model and tokenising triplets\n",
    "\n",
    "\n",
    "def tokenise_triplets(triplets_list):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('m.model')\n",
    "    return [(sp.encode_as_pieces(query), sp.encode_as_pieces(rel_doc), sp.encode_as_pieces(irr_doc))\n",
    "                      for (query, rel_doc, irr_doc) in triplets_list]\n",
    "\n",
    "def generateEmbeddedTriplets(tokenised_triplets):\n",
    "    loaded_model = Word2Vec.load(\"word2vec_model.model\")\n",
    "    return [(encode_and_pool([query], loaded_model)[0], encode_and_pool(rel_doc, loaded_model), encode_and_pool(irr_doc, loaded_model))\n",
    "                      for (query, rel_doc, irr_doc) in tokenised_triplets]\n",
    "\n",
    "\n",
    "def encode_and_pool(sentences, word2vec_model):\n",
    "    # Function to encode sentences into word embeddings and apply average pooling\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Encode each word in the sentence using Word2Vec\n",
    "        word_embeddings = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv.key_to_index]\n",
    "        # Apply average pooling to obtain a fixed-length representation\n",
    "        if word_embeddings:\n",
    "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "            embeddings.append(sentence_embedding)\n",
    "        else:\n",
    "            # If no word embeddings found (out-of-vocabulary words), use zero vector\n",
    "            embeddings.append(np.zeros(word2vec_model.vector_size))\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pre processing\n",
      "Training data loaded\n",
      "Validation data loaded\n",
      "Test data loaded\n",
      "Generating triplets and writing to corpus file\n",
      "Triplets generated - training sentence piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 10801 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1019574\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=92\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 10801 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=582010\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 44252 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 10801\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 26160\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 26160 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17064 obj=11.7298 num_tokens=54210 num_tokens/piece=3.17686\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14679 obj=9.65763 num_tokens=54607 num_tokens/piece=3.72008\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11005 obj=9.69028 num_tokens=58412 num_tokens/piece=5.30777\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11002 obj=9.64261 num_tokens=58417 num_tokens/piece=5.30967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10969 obj=9.64181 num_tokens=58484 num_tokens/piece=5.33175\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10967 obj=9.64227 num_tokens=58545 num_tokens/piece=5.33829\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence piece trainined, now tokenising triplets\n",
      "Triplets tokenised\n",
      "Data preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting pre processing\")\n",
    "# Loading Traning Data \n",
    "training_query_dataset = pd.read_parquet(\"./v1.1-data/train.parquet\") \n",
    "training_query_and_relevant_doc_df = removeRedundantColumns(training_query_dataset).head(100)\n",
    "print(\"Training data loaded\")\n",
    "\n",
    "# Loading Validation Data\n",
    "validation_query_dataset = pd.read_parquet(\"./v1.1-data/validation.parquet\") \n",
    "validation_query_and_relevant_doc_df = removeRedundantColumns(validation_query_dataset).head(100)\n",
    "print(\"Validation data loaded\")\n",
    "\n",
    "# Loading Test Data\n",
    "test_query_dataset = pd.read_parquet(\"./v1.1-data/test.parquet\") \n",
    "test_query_and_relevant_doc_df = removeRedundantColumns(test_query_dataset).head(100)\n",
    "print(\"Test data loaded\")\n",
    "\n",
    "# Creating corpus file if it doesn't exist and removing lingering data from any last runs \n",
    "clearCorpusFile()\n",
    "print(\"Generating triplets and writing to corpus file\")\n",
    "# Creating triplets for each data set and updating all the values to the same corpus\n",
    "training_triplets_list = writeToCorpusAndGenerateTriplets(training_query_and_relevant_doc_df)\n",
    "validation_triplets_list = writeToCorpusAndGenerateTriplets(validation_query_and_relevant_doc_df)\n",
    "test_triplets_list = writeToCorpusAndGenerateTriplets(test_query_and_relevant_doc_df)\n",
    "\n",
    "print(\"Triplets generated - training sentence piece\")\n",
    "# Training sentencepiece on training, validation and test data\n",
    "spm.SentencePieceTrainer.train(input=\"corpus.txt\", model_prefix=\"m\", vocab_size=10_000)\n",
    "print(\"Sentence piece trainined, now tokenising triplets\")\n",
    "\n",
    "# Tokenising triplets using sentencepiece \n",
    "training_tokenised_triplets = tokenise_triplets(training_triplets_list)\n",
    "validation_tokenised_triplets = tokenise_triplets(validation_triplets_list)\n",
    "test_tokenised_triplets = tokenise_triplets(test_triplets_list)\n",
    "print(\"Triplets tokenised\")\n",
    "print(\"Data preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenised triplets into pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# with open('validation_tokenized_triplet.pkl', 'wb') as file:\n",
    "#     pickle.dump(validation_tokenised_triplets, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('test_tokenised_triplets.pkl', 'wb') as file:\n",
    "#     pickle.dump(validation_tokenised_triplets, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "embedded_training_set_values = generateEmbeddedTriplets(training_tokenised_triplets)\n",
    "\n",
    "with open('training_tokenised_triplets.pkl', 'wb') as file:\n",
    "    pickle.dump(embedded_training_set_values, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

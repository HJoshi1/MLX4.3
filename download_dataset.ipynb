{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/harshil/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1 - Get the dataset from MS Marco and put into data frame\n",
    "\n",
    "query_dataset = pd.read_parquet(\"./v1.1-data/\") \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passages</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'is_selected': [0, 0, 1, 0, 0, 0, 0], 'passag...</td>\n",
       "      <td>does human hair stop squirrels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0, 0, 0], '...</td>\n",
       "      <td>what are the benefits of fossil fuels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...</td>\n",
       "      <td>what is a apothem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...</td>\n",
       "      <td>average cost for custom canopy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            passages  \\\n",
       "0  {'is_selected': [0, 0, 1, 0, 0, 0, 0], 'passag...   \n",
       "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0, 0, 0], '...   \n",
       "2  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...   \n",
       "3  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...   \n",
       "\n",
       "                                   query  \n",
       "0         does human hair stop squirrels  \n",
       "1  what are the benefits of fossil fuels  \n",
       "2                      what is a apothem  \n",
       "3         average cost for custom canopy  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query_dataset.head(1).loc[:,\"passages\"][0]['passage_text'][4]\n",
    "# Step 2 - Compile list of all the documenets \n",
    "# - can do as a key, value pair when key is the query id (or query itself) and values are all the docs\n",
    "\n",
    "query_and_relevant_doc_df = query_dataset.drop(columns=[\"answers\", \"query_id\", \"query_type\", \"wellFormedAnswers\"])\n",
    "\n",
    "query_and_relevant_doc_df.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to corpus\n",
      "We have been feeding our back yard squirrels for the fall and winter and we noticed that a few of them have missing fur.\n",
      "One has a patch missing down his back and under both arms.\n",
      "Also another has some missing on his whole chest.\n",
      "They are all eating and seem to have a good appetite.\n",
      "Writing to corpus\n",
      "Critters cannot stand the smell of human hair, so sprinkling a barrier of hair clippings around your garden, or lightly working it into the soil when you plant bulbs, apparently does have some merit.\n",
      "The whole thing kind of makes me laugh.\n",
      "It never occurred to me that we are the ones that stink.\n",
      "Writing to corpus\n",
      "Spread some human hair around your vegetable and flower gardens.\n",
      "This will scare the squirrels away because humans are predators of squirrels.\n",
      "It is better if the hair hasn't been washed so the squirrels will easily pick up the human scent.\n",
      "Writing to corpus\n",
      "1 You can sprinkle blood meal around your garden as well.\n",
      "2  Don’t trap and relocate squirrels.\n",
      "3  This is a losing battle since the population of squirrels is extremely high.\n",
      "4  Also, if the animal is a female there is a high likelihood that you will remove her from babies that depend on her for survival.\n",
      "Writing to corpus\n",
      "Hair loss in squirrels can be caused by mange or fungal disease.\n",
      "Mange is a disease caused by microscopic mites that burrow under the skin.\n",
      "The squirrel mange mite has been reported in both fox and gray squirrels.\n",
      "There have been no reports of this type of mange being transmitted to humans or domestic pets.\n",
      "Writing to corpus\n",
      "1 You can also scatter dog or human hair around your garden.\n",
      "2  One readers shares, “I used to have a problem with squirrels digging up my bulbs.\n",
      "3  Now, once in the spring and once in the fall, I ask my hairdresser to save a big bag of hair for me.\n",
      "4  I lightly dig this into the soil.\n",
      "Writing to corpus\n",
      "Rabbit repellent.\n",
      "Human hair will keep rabbits out of your garden!!\n",
      "Collect hair from your brushes and spread it around your garden!\n",
      "(via Pioneer Thinking).\n",
      "Natural mulch.\n",
      "When woven into a mat, it retains moisture, it deters weeds.\n",
      "In some instances, it can actually reduce soil erosion.\n",
      "(via NPR).\n",
      "Plant fertilizer.\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Generate triplets \n",
    "# - form: (query, relevant_doc, irrelevant_doc)\n",
    "# Relevant docs are stored with the query in the df above\n",
    "# Irrelevant doc is a random document from any other query in the df - IMPORTANT - WE'RE ASSUMING THE OTHER QUERY IS UNRELATED TO THIS ONE\n",
    "\n",
    "# storing in a dictionairy of form (k,v) - k = query, v = [(rel_doc1, irr_doc1), (rel_doc2, irr_doc2)....,] - this is our negative sampling\n",
    "# query_dataset.head(1).loc[:,\"passages\"][0]['passage_text'][4]\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "triplets_dict = {}\n",
    "num_rows = query_and_relevant_doc_df.shape[0]\n",
    "\n",
    "# # Reset file\n",
    "# with open('corpus.txt', 'w') as file:    \n",
    "#     file.write(\"\") \n",
    "\n",
    "\n",
    "for index, row in query_and_relevant_doc_df.iterrows():\n",
    "    query = row[\"query\"]  # Assuming 'query' is the column name for the query text\n",
    "    passages = row[\"passages\"]  # Accessing the dictionary in the 'passages' column\n",
    "    passage_texts = passages[\"passage_text\"]  # Extracting the list of passage texts\n",
    "    relevant_and_irrelevant_doc_pairs_list = []\n",
    "\n",
    "    \n",
    "    \n",
    "    for relevant_document in passage_texts:        \n",
    "        # Randomly select another index\n",
    "        random_row_index = random.randint(0, num_rows - 1)\n",
    "        # Spliting each passage into a sentence \n",
    "        sentences = sent_tokenize(relevant_document)\n",
    "        # appending each sentence into corpus.txt\n",
    "        with open('corpus.txt', 'a') as file:\n",
    "            print(\"Writing to corpus\")\n",
    "            for sentence in sentences:\n",
    "                print(sentence)\n",
    "                file.write(sentence + '\\n')  # Write each sentence on a new line\n",
    "\n",
    "        while index == random_row_index:\n",
    "            random_row_index = random.randint(0, num_rows - 1)\n",
    "\n",
    "        \n",
    "        # Retrieve a passage from the randomly selected row\n",
    "        random_passages = query_and_relevant_doc_df.loc[random_row_index, \"passages\"]\n",
    "        random_passage_texts = random_passages[\"passage_text\"]\n",
    "        \n",
    "        # Optionally, select a random passage text from the selected row\n",
    "        irrelevant_document = random.choice(random_passage_texts)\n",
    "\n",
    "        relevant_and_irrelevant_doc_pairs = (relevant_document, irrelevant_document)\n",
    "        relevant_and_irrelevant_doc_pairs_list.append(relevant_and_irrelevant_doc_pairs)\n",
    "\n",
    "    triplets_dict[query] = relevant_and_irrelevant_doc_pairs_list\n",
    "\n",
    "    \n",
    "\n",
    "# - create triplet for each of the doc values\n",
    "# - randomly choose a doc from other keys\n",
    "# - store somewhere \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Tokenise your generated data\n",
    "# Sentence piece \n",
    "# Step 4.1 Train the model \n",
    "spm.SentencePieceTrainer.train(input=corpus.txt, model_prefix=m, vocab_size=32000)\n",
    "\n",
    "#step 4.2 Load the trained model \n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "tokenized_line = [] \n",
    "\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # tokenise each line \n",
    "        tokenised = sp.encode_as_pieces(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

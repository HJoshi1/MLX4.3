{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Setup Tasks###"
      ],
      "metadata": {
        "id": "OOuUI9tGj-Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Dm0LI2Owf2bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Statements###"
      ],
      "metadata": {
        "id": "UbtUPyCAj4iW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cLNXshrOfWj7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths (only for running on Mark's Mac)\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/Document Ranking')\n",
        "\n",
        "data_dir = os.getcwd()\n",
        "spm_path = os.getcwd() + '/spm.model'\n",
        "w2v_path = os.getcwd() + '/w2v.model'\n",
        "train_set_path = os.getcwd() + '/train-00000-of-00001.parquet'"
      ],
      "metadata": {
        "id": "AJBlBfuxgAaB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Constants and Global Variables###"
      ],
      "metadata": {
        "id": "f0DfWLHrjpQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for data preparation\n",
        "data_max_len = 1000 # 512\n",
        "chunk_size = 1000 # make much larger - set to a multiple of the number of rows in a row group of the parquet file\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "pdo15PJbjxl3"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Class Definitions###"
      ],
      "metadata": {
        "id": "yWVJ4mRbjCjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChunkedTripletDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, parquet_file_path, chunk_size, data_max_len):\n",
        "        super().__init__()\n",
        "        self.parquet_file = pq.ParquetFile(parquet_file_path)\n",
        "        self.chunk_size = chunk_size\n",
        "        self.total_rows = self.parquet_file.metadata.num_rows\n",
        "        self.total_row_groups = self.parquet_file.num_row_groups\n",
        "\n",
        "        # Define the schema for the columns\n",
        "        self.schema = pa.schema([\n",
        "            (\"query\", pa.string()),\n",
        "            (\"passages\", pa.struct([\n",
        "            (\"is_selected\", pa.list_(pa.int32())),\n",
        "            (\"passage_text\", pa.list_(pa.string())),\n",
        "            (\"url\", pa.list_(pa.string()))\n",
        "            ]))\n",
        "        ])\n",
        "\n",
        "        # Print the total number of rows and row groups\n",
        "        print(f'Total number of rows: {self.total_rows}')\n",
        "        print(f'Total number of row groups: {self.total_row_groups}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_rows\n",
        "\n",
        "    def __iter__(self):\n",
        "        start_row = 0\n",
        "        end_row = self.total_rows\n",
        "\n",
        "        for start in range(start_row, end_row, self.chunk_size):\n",
        "            end = min(start + self.chunk_size, end_row)\n",
        "            table_chunk = self.parquet_file.read_rows(start, end, columns=['query', 'passages'])\n",
        "            table = pa.Table.from_batches([table_chunk])\n",
        "\n",
        "            query_list = table['query'].to_pylist()\n",
        "            passages = table['passages'].to_pylist()\n",
        "\n",
        "            neg_doc_list = []\n",
        "            for query, passage_row in zip(query_list, passages):\n",
        "                pos_doc_list = passage_row['passage_text']\n",
        "                print(\"POS DOC LIST CHECK:\", pos_doc_list)\n",
        "\n",
        "                # Randomly select negative documents from other rows\n",
        "                random_index = random.choice([i for i in range(len(query_list)) if query_list[i] != query])\n",
        "                neg_pos_doc_list = passages[random_index]['passage_text']\n",
        "                neg_doc_list.append(neg_pos_doc_list)\n",
        "\n",
        "            # Instantiate the TokenizerAndEmbedder class\n",
        "            spm_path = \"your_spm_model_path.model\"\n",
        "            w2v_path = \"your_word2vec_model_path.model\"\n",
        "            tokenizer_and_embedder = TokenizerAndEmbedder(spm_path, w2v_path)\n",
        "\n",
        "            # Generate dataset\n",
        "            yield generate_triplets(query_list, pos_doc_list, neg_doc_list, tokenizer_and_embedder, data_max_len)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Calculate the start and end indices for the current chunk\n",
        "        #start_index = index * self.chunk_size\n",
        "        #end_index = min((index + 1) * self.chunk_size, self.total_rows)\n",
        "\n",
        "        # Calculate the row group index and row index within the row group\n",
        "        row_group_index = index // self.chunk_size\n",
        "        row_index_within_group = index % self.chunk_size\n",
        "\n",
        "        # Check if the row group index is valid\n",
        "        if row_group_index >= self.total_row_groups:\n",
        "            raise IndexError(f\"Row group index {row_group_index} out of bounds\")\n",
        "\n",
        "        # Read the data chunk from the Parquet file\n",
        "        table_chunk = self.parquet_file.read_row_group(row_group_index, columns=['query', 'passages'])\n",
        "        table = pa.Table.from_batches([table_chunk])\n",
        "\n",
        "        # Extract queries and passages from the data chunk\n",
        "        query_list = table['query'].to_pylist()\n",
        "        passages = table['passages'].to_pylist()\n",
        "\n",
        "        # Initialize lists to store positive and negative documents\n",
        "        pos_doc_list = []\n",
        "        neg_doc_list = []\n",
        "\n",
        "        for query, passage_row in zip(query_list, passages):\n",
        "            pos_doc_list.append(passage_row['passage_text'])\n",
        "\n",
        "            # Randomly select a negative document from another row\n",
        "            random_index = random.choice([i for i in range(len(query_list)) if i != index])\n",
        "            neg_doc_list.append(passages[random_index]['passage_text'])\n",
        "\n",
        "        # Instantiate the TokenizerAndEmbedder class\n",
        "        spm_path = \"your_spm_model_path.model\"\n",
        "        w2v_path = \"your_word2vec_model_path.model\"\n",
        "        tokenizer_and_embedder = TokenizerAndEmbedder(spm_path, w2v_path)\n",
        "\n",
        "        # Generate triplets for the current chunk\n",
        "        return generate_triplets(query_list, pos_doc_list, neg_doc_list, tokenizer_and_embedder, self.data_max_len)\n",
        "\n",
        "\n",
        "class TokenizerAndEmbedder:\n",
        "    def __init__(self, spm_path, w2v_path):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(spm_path)\n",
        "        self.w2v = Word2Vec.load(w2v_path)\n",
        "\n",
        "    def tokenize_and_embed(self, input_tensor):\n",
        "        tokenized_tensor = self.sp.EncodeAsPieces(input_tensor)\n",
        "        embedded_tensor = [self.w2v.wv[word] for word in tokenized_tensor if word in self.w2v.wv]\n",
        "\n",
        "        return embedded_tensor\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "mWDcB36zjBew"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function definitions###"
      ],
      "metadata": {
        "id": "gRKxY4RCi2Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_triplets(queries, pos_docs, neg_docs, tokenizer_and_embedder, data_max_len):\n",
        "\n",
        "    # Truncate queries and documents to data_max_len\n",
        "    queries = [query[:data_max_len] for query in queries]\n",
        "    pos_docs = [[doc[:data_max_len] for doc in docs] for docs in pos_docs]\n",
        "    neg_docs = [[doc[:data_max_len] for doc in docs] for docs in neg_docs]\n",
        "\n",
        "    # Build query_tensor\n",
        "    query_list = [query for query, docs in zip(queries, pos_docs) for _ in docs]\n",
        "    query_tensor = torch.tensor(query_list)\n",
        "\n",
        "    # SBuild pos_doc_tensor\n",
        "    pos_doc_list = [doc for docs in pos_docs for doc in docs]\n",
        "    pos_doc_tensor = torch.tensor(pos_doc_list)\n",
        "\n",
        "    # Build neg_doc_tensor\n",
        "    neg_doc_list = [doc for docs in neg_docs for doc in docs]\n",
        "    neg_doc_tensor = torch.tensor(neg_doc_list)\n",
        "\n",
        "    # Verification\n",
        "    print(\"pos_doc_tensor:\", pos_doc_tensor)\n",
        "    print(\"neg_doc_tensor:\", neg_doc_tensor)\n",
        "    print(\"query_tensor:\", query_tensor)\n",
        "\n",
        "    # tokenize and embed the tensors\n",
        "    pos_doc_tensor_embedded = [tokenizer_and_embedder.tokenize_and_embed(doc) for doc in pos_doc_list]\n",
        "    neg_doc_tensor_embedded = [tokenizer_and_embedder.tokenize_and_embed(doc) for doc in neg_doc_list]\n",
        "\n",
        "    # Sort the tensors\n",
        "    # Calculate the lengths of the documents in pos_doc_tensor and neg_doc_tensor\n",
        "    pos_doc_lengths = [len(doc) for doc in pos_doc_tensor]\n",
        "    neg_doc_lengths = [len(doc) for doc in neg_doc_tensor]\n",
        "\n",
        "    # Determine which tensor has the longest documents\n",
        "    if max(pos_doc_lengths) > max(neg_doc_lengths):\n",
        "        longest_tensor = pos_doc_tensor\n",
        "    else:\n",
        "        longest_tensor = neg_doc_tensor\n",
        "\n",
        "    # Sort the longest tensor along with the other tensors\n",
        "    sorted_indices = torch.argsort(longest_tensor.apply(len))  # Sort indices based on document length\n",
        "    pos_doc_tensor = pos_doc_tensor[sorted_indices]\n",
        "    neg_doc_tensor = neg_doc_tensor[sorted_indices]\n",
        "    query_tensor = query_tensor[sorted_indices]\n",
        "\n",
        "    # Verification\n",
        "    print(\"pos_doc_tensor:\", pos_doc_tensor)\n",
        "    print(\"neg_doc_tensor:\", neg_doc_tensor)\n",
        "    print(\"query_tensor:\", query_tensor)\n",
        "\n",
        "    return (query_tensor, pos_doc_tensor, neg_doc_tensor)\n",
        "\n",
        "\n",
        "def tokenize_and_embed(input_tensor, sp, w2v):\n",
        "    tokenized_tensor = sp(input_tensor)\n",
        "    embedded_tensor = w2v(tokenized_tensor)\n",
        "\n",
        "    return embedded_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "QwfZ27VPiz5O"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main program logic###"
      ],
      "metadata": {
        "id": "L439N0fKhFKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\n",
        "dataset = ChunkedTripletDataset(train_set_path, chunk_size=chunk_size, data_max_len=data_max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6NUyrd4tjLQ",
        "outputId": "9c218b1a-4c61-48b7-cad8-b5ac0ceb6137"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows: 82326\n",
            "Total number of row groups: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the dataloader###"
      ],
      "metadata": {
        "id": "-dhNQXsRYfuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader:\n",
        "    query_tensor, pos_doc_tensor, neg_doc_tensor = batch\n",
        "    print(\"Query tensor size:\", query_tensor.size())\n",
        "    print(\"Positive document tensor size:\", pos_doc_tensor.size())\n",
        "    print(\"Negative document tensor size:\", neg_doc_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "wgTZGNKPYe04",
        "outputId": "1ce85e58-b05f-4be5-c0c3-c5ebb8c4e61e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot convert pyarrow.lib.Table to pyarrow.lib.RecordBatch",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-92d96fae821b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mquery_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_doc_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_doc_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Query tensor size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Positive document tensor size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_doc_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative document tensor size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_doc_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-80a4d22a98f3>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Read the data chunk from the Parquet file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtable_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_row_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_group_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'passages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Extract queries and passages from the data chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_batches\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert pyarrow.lib.Table to pyarrow.lib.RecordBatch"
          ]
        }
      ]
    }
  ]
}